#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆæ•°æ®é›†åˆ†å‰²è„šæœ¬
ä½¿ç”¨æ–°çš„åˆ†å‰²æ¯”ä¾‹é‡æ–°ç”Ÿæˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ†å‰²
"""

import json
import random
from pathlib import Path
from typing import Dict, List, Tuple

def regenerate_dataset_splits(
    input_json: str = "json/master_sharpness_averaged_dataset.json",
    output_dir: str = "json",
    train_ratio: float = 0.7,
    val_ratio: float = 0.2,
    test_ratio: float = 0.1,
    seed: int = 42
):
    """
    é‡æ–°ç”Ÿæˆæ•°æ®é›†åˆ†å‰²
    
    Args:
        input_json: è¾“å…¥çš„master JSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    """
    
    # éªŒè¯åˆ†å‰²æ¯”ä¾‹
    total_ratio = train_ratio + val_ratio + test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        raise ValueError(f"åˆ†å‰²æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º: {total_ratio}")
    
    print(f"å¼€å§‹é‡æ–°ç”Ÿæˆæ•°æ®é›†åˆ†å‰²...")
    print(f"è¾“å…¥æ–‡ä»¶: {input_json}")
    print(f"åˆ†å‰²æ¯”ä¾‹: è®­ç»ƒ{train_ratio:.1%}, éªŒè¯{val_ratio:.1%}, æµ‹è¯•{test_ratio:.1%}")
    print(f"éšæœºç§å­: {seed}")
    
    # è¯»å–è¾“å…¥JSON
    input_path = Path(input_json)
    if not input_path.exists():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_json}")
    
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    samples = data.get("samples", [])
    datasets_info = data.get("datasets_info", {})
    total_samples = len(samples)
    
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    
    # ç”Ÿæˆéšæœºç´¢å¼•
    indices = list(range(total_samples))
    random.shuffle(indices)
    
    # è®¡ç®—å„é›†åˆçš„æ ·æœ¬æ•°é‡
    n_train = int(total_samples * train_ratio)
    n_val = int(total_samples * val_ratio)
    n_test = total_samples - n_train - n_val
    
    print(f"åˆ†å‰²ç»“æœ: è®­ç»ƒ{n_train}å¼ , éªŒè¯{n_val}å¼ , æµ‹è¯•{n_test}å¼ ")
    
    # åˆ†å‰²æ ·æœ¬
    train_indices = indices[:n_train]
    val_indices = indices[n_train:n_train + n_val]
    test_indices = indices[n_train + n_val:]
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆè®­ç»ƒé›†JSON
    train_samples = [samples[i] for i in train_indices]
    train_data = {
        "samples": train_samples,
        "datasets_info": datasets_info,
        "description": f"è®­ç»ƒé›† - æ¸…æ™°åº¦å¹³å‡åŒ–åçš„æœ‰ç›‘ç£åˆ†å‰²è®­ç»ƒæ ·æœ¬ (åˆ†å‰²æ¯”ä¾‹: {train_ratio:.1%})",
        "num_samples": len(train_samples),
        "split_info": {
            "split": "train",
            "ratio": train_ratio,
            "seed": seed
        }
    }
    
    train_json_path = output_path / "master_sharpness_averaged_dataset_train.json"
    with open(train_json_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è®­ç»ƒé›†JSONå·²ç”Ÿæˆ: {train_json_path} ({len(train_samples)}å¼ )")
    
    # ç”ŸæˆéªŒè¯é›†JSON
    val_samples = [samples[i] for i in val_indices]
    val_data = {
        "samples": val_samples,
        "datasets_info": datasets_info,
        "description": f"éªŒè¯é›† - æ¸…æ™°åº¦å¹³å‡åŒ–åçš„æœ‰ç›‘ç£åˆ†å‰²è®­ç»ƒæ ·æœ¬ (åˆ†å‰²æ¯”ä¾‹: {val_ratio:.1%})",
        "num_samples": len(val_samples),
        "split_info": {
            "split": "val",
            "ratio": val_ratio,
            "seed": seed
        }
    }
    
    val_json_path = output_path / "master_sharpness_averaged_dataset_val.json"
    with open(val_json_path, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… éªŒè¯é›†JSONå·²ç”Ÿæˆ: {val_json_path} ({len(val_samples)}å¼ )")
    
    # ç”Ÿæˆæµ‹è¯•é›†JSON
    test_samples = [samples[i] for i in test_indices]
    test_data = {
        "samples": test_samples,
        "datasets_info": datasets_info,
        "description": f"æµ‹è¯•é›† - æ¸…æ™°åº¦å¹³å‡åŒ–åçš„æœ‰ç›‘ç£åˆ†å‰²è®­ç»ƒæ ·æœ¬ (åˆ†å‰²æ¯”ä¾‹: {test_ratio:.1%})",
        "num_samples": len(test_samples),
        "split_info": {
            "split": "test",
            "ratio": test_ratio,
            "seed": seed
        }
    }
    
    test_json_path = output_path / "master_sharpness_averaged_dataset_test.json"
    with open(test_json_path, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æµ‹è¯•é›†JSONå·²ç”Ÿæˆ: {test_json_path} ({len(test_samples)}å¼ )")
    
    # ç”Ÿæˆåˆ†å‰²æ‘˜è¦
    summary_data = {
        "original_file": str(input_json),
        "total_samples": total_samples,
        "split_ratios": {
            "train": train_ratio,
            "val": val_ratio,
            "test": test_ratio
        },
        "split_counts": {
            "train": n_train,
            "val": n_val,
            "test": n_test
        },
        "output_files": {
            "train": str(train_json_path),
            "val": str(val_json_path),
            "test": str(test_json_path)
        },
        "random_seed": seed,
        "description": "æ•°æ®é›†åˆ†å‰²é‡æ–°ç”Ÿæˆç»“æœ"
    }
    
    summary_path = output_path / "dataset_split_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š åˆ†å‰²æ‘˜è¦å·²ç”Ÿæˆ: {summary_path}")
    
    # æ‰“å°æ ·æœ¬åˆ†å¸ƒ
    print(f"\n=== æ ·æœ¬åˆ†å¸ƒ ===")
    print(f"è®­ç»ƒé›†: {n_train}å¼  ({n_train/total_samples:.1%})")
    print(f"éªŒè¯é›†: {n_val}å¼  ({n_val/total_samples:.1%})")
    print(f"æµ‹è¯•é›†: {n_test}å¼  ({n_test/total_samples:.1%})")
    
    # æŒ‰æ•°æ®é›†ç»Ÿè®¡
    print(f"\n=== æŒ‰æ•°æ®é›†ç»Ÿè®¡ ===")
    for split_name, split_samples in [("è®­ç»ƒé›†", train_samples), ("éªŒè¯é›†", val_samples), ("æµ‹è¯•é›†", test_samples)]:
        dataset_counts = {}
        for sample in split_samples:
            dataset = sample.get("dataset", "unknown")
            dataset_counts[dataset] = dataset_counts.get(dataset, 0) + 1
        
        print(f"{split_name}:")
        for dataset, count in dataset_counts.items():
            print(f"  {dataset}: {count}å¼ ")
    
    print(f"\nğŸ‰ æ•°æ®é›†åˆ†å‰²é‡æ–°ç”Ÿæˆå®Œæˆï¼")
    return {
        "train": train_json_path,
        "val": val_json_path,
        "test": test_json_path,
        "summary": summary_path
    }

def main():
    """ä¸»å‡½æ•°"""
    try:
        # ä½¿ç”¨æ–°çš„åˆ†å‰²æ¯”ä¾‹é‡æ–°ç”Ÿæˆæ•°æ®é›†åˆ†å‰²
        result = regenerate_dataset_splits(
            input_json="json/master_sharpness_averaged_dataset.json",
            output_dir="json",
            train_ratio=0.7,  # è®­ç»ƒ70%
            val_ratio=0.2,    # éªŒè¯20%
            test_ratio=0.1,   # æµ‹è¯•10%
            seed=42
        )
        
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
        for split, path in result.items():
            if split != "summary":
                print(f"  {split}: {path}")
        
        print(f"\nç°åœ¨æ‚¨å¯ä»¥:")
        print(f"1. ä½¿ç”¨æ–°çš„éªŒè¯é›†JSONè¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°")
        print(f"2. é‡æ–°è®­ç»ƒæ¨¡å‹ä½¿ç”¨æ–°çš„è®­ç»ƒé›†")
        print(f"3. åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ€§èƒ½")
        
    except Exception as e:
        print(f"âŒ é‡æ–°ç”Ÿæˆæ•°æ®é›†åˆ†å‰²å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
