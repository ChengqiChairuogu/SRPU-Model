"""
å›¾åƒæ¸…æ™°åº¦å¤„ç†å™¨
ä¸»è¦åŠŸèƒ½ï¼š
1. è®¡ç®—æ¯å¼ å›¾ç‰‡çš„æ¸…æ™°åº¦ï¼Œç”Ÿæˆåˆ†æå›¾è¡¨å’ŒJSONæ–‡ä»¶
2. å¯¹å›¾åƒæ¸…æ™°åº¦è¿›è¡Œå¹³å‡åŒ–å¤„ç†ï¼Œè®©æ‰€æœ‰å›¾åƒçš„æ¸…æ™°åº¦åŸºæœ¬ç›¸ç­‰
3. å°†å¤„ç†åçš„å›¾åƒä¿å­˜åˆ°å„è‡ªæ•°æ®é›†ç›®å½•ä¸‹
4. ä¸ºæ–°çš„æ•°æ®é›†ç”ŸæˆJSONæ–‡ä»¶ç”¨äºè®­ç»ƒ
"""

import json
import logging
import sys
import os
from pathlib import Path, WindowsPath
from typing import Dict, List, Optional, Tuple
import cv2
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from datetime import datetime

# è®¾ç½®ä¸­æ–‡è·¯å¾„æ”¯æŒ
if sys.platform == "win32":
    # Windowsç³»ç»Ÿä¸­æ–‡è·¯å¾„æ”¯æŒ
    import locale
    try:
        # å°è¯•è®¾ç½®ç³»ç»Ÿé»˜è®¤ç¼–ç 
        locale.setlocale(locale.LC_ALL, '')
    except:
        pass
    
    # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

from configs.image_sharpness.sharpness_processor_config import *
from utils.image_sharpness_util import (
    SharpnessAssessor, 
    apply_sharpness_averaging,
    calculate_target_sharpness,
    find_reference_image,
    save_image_to_dataset
)
from utils.analysis_chart_util import SharpnessAnalysisChartGenerator


def safe_path_operation(func):
    """è£…é¥°å™¨ï¼šå®‰å…¨å¤„ç†ä¸­æ–‡è·¯å¾„"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except (OSError, UnicodeEncodeError, UnicodeDecodeError) as e:
            print(f"è·¯å¾„æ“ä½œé”™è¯¯: {e}")
            print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            print(f"Pythonç‰ˆæœ¬: {sys.version}")
            print(f"å¹³å°: {sys.platform}")
            raise
    return wrapper


class ImageSharpnessProcessor:
    """å›¾åƒæ¸…æ™°åº¦å¤„ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.logger = self._setup_logging()
        
        # åˆå§‹åŒ–æ¸…æ™°åº¦è¯„ä¼°å™¨
        self.assessor = SharpnessAssessor(methods=SHARPNESS_METHODS)
        
        # åˆå§‹åŒ–å›¾è¡¨ç”Ÿæˆå™¨
        self.chart_generator = SharpnessAnalysisChartGenerator()
        
        self.logger.info("å›¾åƒæ¸…æ™°åº¦å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _safe_path(self, path_str: str) -> Path:
        """å®‰å…¨å¤„ç†ä¸­æ–‡è·¯å¾„"""
        try:
            # å°è¯•ç›´æ¥åˆ›å»ºPathå¯¹è±¡
            path = Path(path_str)
            return path
        except Exception as e:
            self.logger.warning(f"è·¯å¾„åˆ›å»ºå¤±è´¥: {path_str}, é”™è¯¯: {e}")
            try:
                # å°è¯•ä½¿ç”¨ç»å¯¹è·¯å¾„
                if os.path.isabs(path_str):
                    path = Path(os.path.abspath(path_str))
                else:
                    path = Path(os.path.abspath(path_str))
                return path
            except Exception as e2:
                self.logger.error(f"ç»å¯¹è·¯å¾„åˆ›å»ºä¹Ÿå¤±è´¥: {path_str}, é”™è¯¯: {e2}")
                # æœ€åå°è¯•ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„
                return Path(str(path_str))
    
    def _safe_image_open(self, image_path: Path) -> np.ndarray:
        """å®‰å…¨æ‰“å¼€å›¾åƒæ–‡ä»¶"""
        try:
            # å°è¯•ä½¿ç”¨PILæ‰“å¼€
            with Image.open(str(image_path)) as img:
                return np.array(img)
        except Exception as e:
            self.logger.warning(f"PILæ‰“å¼€å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
            try:
                # å°è¯•ä½¿ç”¨OpenCVæ‰“å¼€
                img = cv2.imread(str(image_path))
                if img is not None:
                    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                else:
                    raise ValueError(f"OpenCVæ— æ³•è¯»å–å›¾åƒ: {image_path}")
            except Exception as e2:
                self.logger.error(f"OpenCVæ‰“å¼€å›¾åƒä¹Ÿå¤±è´¥: {image_path}, é”™è¯¯: {e2}")
                raise
    
    def process_images(self) -> None:
        """å¤„ç†å›¾åƒçš„ä¸»æµç¨‹"""
        self.logger.info("å¼€å§‹å¤„ç†å›¾åƒæ¸…æ™°åº¦...")
        
        # 1. è¯„ä¼°åŸå§‹å›¾åƒæ¸…æ™°åº¦
        self.logger.info("æ­¥éª¤1: è¯„ä¼°åŸå§‹å›¾åƒæ¸…æ™°åº¦")
        original_sharpness = self._assess_original_images()
        
        # 2. ç”ŸæˆåŸå§‹æ¸…æ™°åº¦åˆ†æå›¾è¡¨
        if GENERATE_BEFORE_CHARTS:
            self.logger.info("æ­¥éª¤2: ç”ŸæˆåŸå§‹æ¸…æ™°åº¦åˆ†æå›¾è¡¨")
            self._generate_original_analysis_charts(original_sharpness)
        
        # 3. ä¿å­˜æ¸…æ™°åº¦æ•°æ®åˆ°JSON
        self.logger.info("æ­¥éª¤3: ä¿å­˜æ¸…æ™°åº¦æ•°æ®åˆ°JSON")
        self._save_sharpness_json(original_sharpness, "original")
        
        # 4. åº”ç”¨æ¸…æ™°åº¦å¹³å‡åŒ–å¤„ç†
        if ENABLE_SHARPNESS_AVERAGING:
            self.logger.info("æ­¥éª¤4: åº”ç”¨æ¸…æ™°åº¦å¹³å‡åŒ–å¤„ç†")
            self._apply_sharpness_averaging(original_sharpness)
        
        # 5. è¯„ä¼°å¤„ç†åçš„å›¾åƒæ¸…æ™°åº¦
        self.logger.info("æ­¥éª¤5: è¯„ä¼°å¤„ç†åçš„å›¾åƒæ¸…æ™°åº¦")
        processed_sharpness = self._assess_processed_images()
        
        # 6. ç”Ÿæˆå¤„ç†åçš„æ¸…æ™°åº¦åˆ†æå›¾è¡¨
        if GENERATE_AFTER_CHARTS:
            self.logger.info("æ­¥éª¤6: ç”Ÿæˆå¤„ç†åçš„æ¸…æ™°åº¦åˆ†æå›¾è¡¨")
            self._generate_processed_analysis_charts(original_sharpness, processed_sharpness)
        
        # 7. ä¿å­˜å¤„ç†åçš„æ¸…æ™°åº¦æ•°æ®åˆ°JSON
        self.logger.info("æ­¥éª¤7: ä¿å­˜å¤„ç†åçš„æ¸…æ™°åº¦æ•°æ®åˆ°JSON")
        self._save_sharpness_json(processed_sharpness, "processed")
        
        # 8. ç”Ÿæˆè®­ç»ƒç”¨JSONæ–‡ä»¶
        if GENERATE_TRAINING_JSON or GENERATE_SSL_JSON:
            self.logger.info("æ­¥éª¤8: ç”Ÿæˆè®­ç»ƒç”¨JSONæ–‡ä»¶")
            self._generate_training_jsons()
        
        # 9. éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶
        self.logger.info("æ­¥éª¤9: éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶")
        validation_success = self._validate_generated_jsons()
        
        if validation_success:
            self.logger.info("âœ… æ‰€æœ‰JSONæ–‡ä»¶éªŒè¯é€šè¿‡ï¼")
        else:
            self.logger.warning("âš ï¸ éƒ¨åˆ†JSONæ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
        
        self.logger.info("å›¾åƒæ¸…æ™°åº¦å¤„ç†å®Œæˆï¼")
    
    def _assess_original_images(self) -> Dict[str, Dict[str, float]]:
        """è¯„ä¼°åŸå§‹å›¾åƒæ¸…æ™°åº¦"""
        if USE_DATASET_JSON and DATASET_JSON_PATH.exists():
            self.logger.info(f"ä½¿ç”¨æ•°æ®é›†JSONæ–‡ä»¶æ¨¡å¼: {DATASET_JSON_PATH}")
            self.logger.info("æ³¨æ„ï¼šæ­¤æ¨¡å¼åªå¤„ç†æœ‰å¯¹åº”maskçš„å›¾åƒ")
            sharpness_data = self._assess_images_from_json()
        else:
            self.logger.info(f"ä½¿ç”¨æ–‡ä»¶å¤¹æ‰«ææ¨¡å¼ï¼Œå¤„ç†æ‰€æœ‰åŸå§‹å›¾åƒ")
            self.logger.info("æ­¤æ¨¡å¼å°†å¤„ç†æ‰€æœ‰æ•°æ®é›†ä¸­çš„æ‰€æœ‰åŸå§‹å›¾åƒ")
            sharpness_data = self._assess_images_from_folder()
        
        self.logger.info(f"æˆåŠŸè¯„ä¼° {len(sharpness_data)} å¼ å›¾åƒ")
        return sharpness_data
    
    def _assess_images_from_json(self) -> Dict[str, Dict[str, float]]:
        """ä»JSONæ–‡ä»¶è¯»å–å›¾åƒåˆ—è¡¨å¹¶è¯„ä¼°æ¸…æ™°åº¦"""
        try:
            with open(DATASET_JSON_PATH, 'r', encoding='utf-8') as f:
                dataset_data = json.load(f)
            
            sharpness_data = {}
            total_samples = len(dataset_data.get("samples", []))
            
            for i, sample in enumerate(dataset_data.get("samples", [])):
                # è·å–æ•°æ®é›†åç§°å’Œå›¾åƒæ–‡ä»¶è·¯å¾„
                dataset_name = sample.get("dataset", "dataset1_LInCl")
                frames = sample.get("frames", [])
                
                for frame in frames:
                    # æ ¹æ®æ•°æ®é›†åç§°é€‰æ‹©å¯¹åº”çš„è¾“å…¥æ–‡ä»¶å¤¹
                    if dataset_name in DATASET_INPUT_PATHS:
                        img_path = DATASET_INPUT_PATHS[dataset_name] / frame
                    elif DATASET_IMAGE_FOLDER.exists():
                        img_path = DATASET_IMAGE_FOLDER / frame
                    else:
                        img_path = INPUT_FOLDER / frame
                    
                    if img_path.exists():
                        try:
                            # è¯„ä¼°å›¾åƒæ¸…æ™°åº¦
                            metrics = self.assessor.assess_image(img_path)
                            sharpness_data[str(img_path)] = metrics
                            
                            # æ˜¾ç¤ºè¿›åº¦
                            if (i + 1) % 10 == 0:
                                self.logger.info(f"å·²å¤„ç† {i + 1}/{total_samples} ä¸ªæ ·æœ¬")
                                
                        except Exception as e:
                            self.logger.warning(f"è¯„ä¼°å›¾åƒ {img_path} å¤±è´¥: {e}")
                    else:
                        self.logger.warning(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            
            return sharpness_data
            
        except Exception as e:
            self.logger.error(f"è¯»å–æ•°æ®é›†JSONæ–‡ä»¶å¤±è´¥: {e}")
            self.logger.info("å›é€€åˆ°æ–‡ä»¶å¤¹æ‰«ææ¨¡å¼")
            return self._assess_images_from_folder()
    
    def _assess_images_from_folder(self) -> Dict[str, Dict[str, float]]:
        """ä»æ–‡ä»¶å¤¹æ‰«æå›¾åƒå¹¶è¯„ä¼°æ¸…æ™°åº¦"""
        sharpness_data = {}
        total_images = 0
        
        # å¦‚æœé…ç½®äº†å¤šæ•°æ®é›†è¾“å…¥è·¯å¾„ï¼Œåˆ™æ‰«ææ‰€æœ‰æ•°æ®é›†
        try:
            from configs.image_sharpness.sharpness_processor_config import DATASET_INPUT_PATHS
            
            self.logger.info(f"å¼€å§‹æ‰«æ {len(DATASET_INPUT_PATHS)} ä¸ªæ•°æ®é›†...")
            
            for dataset_name, input_path in DATASET_INPUT_PATHS.items():
                if input_path.exists():
                    self.logger.info(f"æ‰«ææ•°æ®é›† {dataset_name}: {input_path}")
                    
                    # ç»Ÿè®¡å›¾åƒæ–‡ä»¶æ•°é‡
                    image_files = list(input_path.glob("*.png"))
                    dataset_image_count = len(image_files)
                    total_images += dataset_image_count
                    
                    self.logger.info(f"  å‘ç° {dataset_image_count} å¼ PNGå›¾åƒ")
                    
                    # è¯„ä¼°æ•°æ®é›†ä¸­çš„å›¾åƒ
                    dataset_data = self.assessor.assess_batch(input_path)
                    sharpness_data.update(dataset_data)
                    
                    self.logger.info(f"  æˆåŠŸè¯„ä¼° {len(dataset_data)} å¼ å›¾åƒ")
                else:
                    self.logger.warning(f"æ•°æ®é›†è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
            
            self.logger.info(f"æ–‡ä»¶å¤¹æ‰«æå®Œæˆï¼Œæ€»å…±å¤„ç† {total_images} å¼ å›¾åƒ")
            
        except ImportError:
            # å›é€€åˆ°åŸæ¥çš„å•æ–‡ä»¶å¤¹æ‰«æ
            self.logger.info(f"å›é€€åˆ°å•æ–‡ä»¶å¤¹æ‰«ææ¨¡å¼: {INPUT_FOLDER}")
            
            if not INPUT_FOLDER.exists():
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {INPUT_FOLDER}")
            
            # ç»Ÿè®¡å›¾åƒæ–‡ä»¶æ•°é‡
            image_files = list(INPUT_FOLDER.glob("*.png"))
            total_images = len(image_files)
            self.logger.info(f"å‘ç° {total_images} å¼ PNGå›¾åƒ")
            
            # ä½¿ç”¨SharpnessAssessorè¯„ä¼°å›¾åƒ
            sharpness_data = self.assessor.assess_batch(INPUT_FOLDER)
            self.logger.info(f"æˆåŠŸè¯„ä¼° {len(sharpness_data)} å¼ å›¾åƒ")
        
        return sharpness_data
    
    def _generate_original_analysis_charts(self, sharpness_data: Dict[str, Dict[str, float]]) -> None:
        """ç”ŸæˆåŸå§‹æ¸…æ™°åº¦åˆ†æå›¾è¡¨"""
        chart_dir = CHART_OUTPUT_FOLDER / "before_processing"
        chart_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå„ç§åˆ†æå›¾è¡¨
        self.chart_generator.generate_sharpness_distribution(
            sharpness_data, 
            save_path=chart_dir / "sharpness_distribution.png"
        )
        
        self.chart_generator.generate_sharpness_comparison(
            sharpness_data,
            save_path=chart_dir / "sharpness_comparison.png"
        )
        
        self.chart_generator.generate_sharpness_statistics(
            sharpness_data,
            save_path=chart_dir / "sharpness_statistics.png"
        )
        
        self.logger.info(f"åŸå§‹æ¸…æ™°åº¦åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_dir}")
    
    def _save_sharpness_json(self, sharpness_data: Dict[str, Dict[str, float]], 
                            suffix: str) -> None:
        """ä¿å­˜æ¸…æ™°åº¦æ•°æ®åˆ°JSONæ–‡ä»¶"""
        # å‡†å¤‡JSONæ•°æ®
        json_data = {
            "metadata": {
                "total_images": len(sharpness_data),
                "assessment_methods": SHARPNESS_METHODS,
                "timestamp": str(datetime.now()),
                "type": suffix
            },
            "images": {}
        }
        
        for img_path, metrics in sharpness_data.items():
            # æå–æ–‡ä»¶å
            img_name = Path(img_path).name
            json_data["images"][img_name] = {
                "file_path": str(img_path),
                "metrics": metrics
            }
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_path = JSON_OUTPUT_PATH.parent / f"sharpness_analysis_{suffix}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ¸…æ™°åº¦æ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
    
    def _apply_sharpness_averaging(self, sharpness_data: Dict[str, Dict[str, float]]) -> None:
        """åº”ç”¨æ¸…æ™°åº¦å¹³å‡åŒ–å¤„ç†"""
        # è®¡ç®—ç›®æ ‡æ¸…æ™°åº¦
        target_sharpness = calculate_target_sharpness(
            sharpness_data, 
            AVERAGING_TARGET, 
            CUSTOM_TARGET_SHARPNESS
        )
        
        self.logger.info(f"ç›®æ ‡æ¸…æ™°åº¦: {target_sharpness:.4f}")
        
        # æ‰¾åˆ°å‚è€ƒå›¾åƒï¼ˆç”¨äºç›´æ–¹å›¾åŒ¹é…ï¼‰
        reference_image_path = None
        current_method = AVERAGING_METHOD
        if current_method == "histogram_matching":
            reference_image_path = find_reference_image(sharpness_data, target_sharpness)
            if reference_image_path:
                self.logger.info(f"å‚è€ƒå›¾åƒ: {Path(reference_image_path).name}")
            else:
                self.logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„å‚è€ƒå›¾åƒï¼Œå°†ä½¿ç”¨è‡ªé€‚åº”å‡è¡¡åŒ–æ–¹æ³•")
                current_method = "adaptive_equalization"
        
        # å¤„ç†æ¯å¼ å›¾åƒ
        total_images = len(sharpness_data)
        for i, (img_path, metrics) in enumerate(sharpness_data.items()):
            try:
                self.logger.info(f"å¤„ç†å›¾åƒ {i+1}/{total_images}: {Path(img_path).name}")
                
                # è¯»å–å›¾åƒ
                image = self._read_image(img_path)
                if image is None:
                    continue
                
                # åº”ç”¨æ¸…æ™°åº¦å¹³å‡åŒ–
                processed_image = self._process_single_image(
                    image, 
                    target_sharpness, 
                    reference_image_path,
                    current_method
                )
                
                # ä¿å­˜åˆ°å¯¹åº”æ•°æ®é›†ç›®å½•
                self._save_to_dataset(processed_image, img_path)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†å›¾åƒ {img_path} å¤±è´¥: {e}")
        
        self.logger.info("æ¸…æ™°åº¦å¹³å‡åŒ–å¤„ç†å®Œæˆ")
    
    def _read_image(self, img_path: str) -> Optional[np.ndarray]:
        """è¯»å–å›¾åƒ"""
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(img_path, cv2.IMREAD_COLOR)
            if image is None:
                self.logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                return None
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # å½’ä¸€åŒ–åˆ°[0, 1]
            image = image.astype(np.float32) / 255.0
            
            return image
            
        except Exception as e:
            self.logger.error(f"è¯»å–å›¾åƒå¤±è´¥: {e}")
            return None
    
    def _process_single_image(self, image: np.ndarray, target_sharpness: float, 
                             reference_image_path: Optional[str] = None, 
                             method: str = None) -> np.ndarray:
        """å¤„ç†å•å¼ å›¾åƒ"""
        # å‡†å¤‡å‚è€ƒå›¾åƒï¼ˆå¦‚æœä½¿ç”¨ç›´æ–¹å›¾åŒ¹é…ï¼‰
        reference_image = None
        if reference_image_path and method == "histogram_matching":
            reference_image = self._read_image(reference_image_path)
        
        # åº”ç”¨æ¸…æ™°åº¦å¹³å‡åŒ–
        processed_image = apply_sharpness_averaging(
            image=image,
            target_sharpness=target_sharpness,
            method=method,
            reference_image=reference_image
        )
        
        return processed_image
    
    def _save_to_dataset(self, image: np.ndarray, original_path: str) -> None:
        """ä¿å­˜å›¾åƒåˆ°å¯¹åº”æ•°æ®é›†ç›®å½•"""
        # ç¡®å®šæ•°æ®é›†åç§°
        dataset_name = self._determine_dataset_name(original_path)
        
        if dataset_name in DATASET_OUTPUT_PATHS:
            output_dir = DATASET_OUTPUT_PATHS[dataset_name]
            
            # ä¿å­˜å›¾åƒ
            output_path = save_image_to_dataset(
                image=image,
                original_path=original_path,
                dataset_name=dataset_name,
                output_dir=output_dir
            )
            
            self.logger.info(f"å›¾åƒå·²ä¿å­˜åˆ°: {output_path}")
        else:
            self.logger.warning(f"æœªæ‰¾åˆ°æ•°æ®é›† {dataset_name} çš„è¾“å‡ºè·¯å¾„")
    
    def _determine_dataset_name(self, image_path: str) -> str:
        """æ ¹æ®å›¾åƒè·¯å¾„ç¡®å®šæ•°æ®é›†åç§°"""
        # ä»è·¯å¾„ä¸­æå–æ•°æ®é›†åç§°
        path_parts = Path(image_path).parts
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.logger.debug(f"è§£æå›¾åƒè·¯å¾„: {image_path}")
        self.logger.debug(f"è·¯å¾„éƒ¨åˆ†: {path_parts}")
        
        # ä»é…ç½®æ–‡ä»¶å¯¼å…¥æ•°æ®é›†åç§°åˆ—è¡¨
        try:
            from configs.image_sharpness.sharpness_processor_config import DATASET_NAMES
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç¡¬ç¼–ç çš„é»˜è®¤åˆ—è¡¨
            DATASET_NAMES = ["dataset1_LInCl", "dataset2_LPSCl", "dataset3_LNOCl"]
        
        # æŸ¥æ‰¾åŒ…å«"dataset"çš„è·¯å¾„éƒ¨åˆ†ï¼Œä¼˜å…ˆæŸ¥æ‰¾å®Œæ•´çš„æ•°æ®é›†åç§°
        for part in path_parts:
            if "dataset" in part.lower():
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ•°æ®é›†åç§°
                if any(dataset_name in part for dataset_name in DATASET_NAMES):
                    self.logger.debug(f"ä»è·¯å¾„éƒ¨åˆ†è¯†åˆ«åˆ°å®Œæ•´æ•°æ®é›†: {part}")
                    return part
                elif part == "datasets":
                    # å¦‚æœåªæ˜¯"datasets"ç›®å½•ï¼Œç»§ç»­æŸ¥æ‰¾
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„æ•°æ®é›†åç§°ï¼Œå°è¯•ä»æ–‡ä»¶åæ¨æ–­
        filename = Path(image_path).name
        self.logger.debug(f"ä»æ–‡ä»¶åæ¨æ–­: {filename}")
        
        # æ ¹æ®æ–‡ä»¶åç‰¹å¾è¯†åˆ«æ•°æ®é›†
        if "4.2V" in filename or "4.4V" in filename:
            self.logger.debug(f"è¯†åˆ«ä¸º dataset1_LInCl (åŸºäºæ–‡ä»¶å: {filename})")
            return "dataset1_LInCl"
        elif "LPSCl" in filename:
            self.logger.debug(f"è¯†åˆ«ä¸º dataset2_LPSCl (åŸºäºæ–‡ä»¶å: {filename})")
            return "dataset2_LPSCl"
        elif "LNOCl" in filename:
            self.logger.debug(f"è¯†åˆ«ä¸º dataset3_LNOCl (åŸºäºæ–‡ä»¶å: {filename})")
            return "dataset3_LNOCl"
        else:
            # å¦‚æœè¿˜æ˜¯æ— æ³•è¯†åˆ«ï¼Œå°è¯•ä»å®Œæ•´è·¯å¾„æ¨æ–­
            path_str = str(image_path)
            self.logger.debug(f"ä»å®Œæ•´è·¯å¾„æ¨æ–­: {path_str}")
            
            if "dataset1" in path_str or "LInCl" in path_str:
                self.logger.debug(f"è¯†åˆ«ä¸º dataset1_LInCl (åŸºäºå®Œæ•´è·¯å¾„)")
                return "dataset1_LInCl"
            elif "dataset2" in path_str or "LPSCl" in path_str:
                self.logger.debug(f"è¯†åˆ«ä¸º dataset2_LPSCl (åŸºäºå®Œæ•´è·¯å¾„)")
                return "dataset2_LPSCl"
            elif "dataset3" in path_str or "LNOCl" in path_str:
                self.logger.debug(f"è¯†åˆ«ä¸º dataset3_LNOCl (åŸºäºå®Œæ•´è·¯å¾„)")
                return "dataset3_LNOCl"
            else:
                # æœ€åé»˜è®¤è¿”å›dataset1
                self.logger.warning(f"æ— æ³•è¯†åˆ«æ•°æ®é›†åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼ dataset1_LInCl: {image_path}")
                return "dataset1_LInCl"
    
    def _assess_processed_images(self) -> Dict[str, Dict[str, float]]:
        """è¯„ä¼°å¤„ç†åçš„å›¾åƒæ¸…æ™°åº¦"""
        # æ”¶é›†æ‰€æœ‰å¤„ç†åçš„å›¾åƒè·¯å¾„
        processed_images = []
        for dataset_name, output_dir in DATASET_OUTPUT_PATHS.items():
            if output_dir.exists():
                for img_file in output_dir.glob("*.png"):
                    processed_images.append(str(img_file))
        
        if not processed_images:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¤„ç†åçš„å›¾åƒ")
            return {}
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹ç”¨äºè¯„ä¼°
        temp_dir = Path("temp_processed_images")
        temp_dir.mkdir(exist_ok=True)
        
        try:
            # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹
            for img_path in processed_images:
                if Path(img_path).exists():
                    temp_path = temp_dir / Path(img_path).name
                    if not temp_path.exists():
                        temp_path.symlink_to(Path(img_path).absolute())
            
            # è¯„ä¼°æ¸…æ™°åº¦
            sharpness_data = self.assessor.assess_batch(temp_dir)
            
            self.logger.info(f"æˆåŠŸè¯„ä¼° {len(sharpness_data)} å¼ å¤„ç†åçš„å›¾åƒ")
            return sharpness_data
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤¹
            import shutil
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
    
    def _generate_processed_analysis_charts(self, original_data: Dict[str, Dict[str, float]], 
                                          processed_data: Dict[str, Dict[str, float]]) -> None:
        """ç”Ÿæˆå¤„ç†åæ¸…æ™°åº¦åˆ†æå›¾è¡¨"""
        chart_dir = CHART_OUTPUT_FOLDER / "after_processing"
        chart_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå¤„ç†å‰åå¯¹æ¯”å›¾
        self.chart_generator.generate_before_after_comparison(
            original_data, 
            processed_data,
            save_path=chart_dir / "before_after_comparison.png"
        )
        
        # ç”Ÿæˆå¤„ç†åæ¸…æ™°åº¦ä¸€è‡´æ€§åˆ†æ
        self.chart_generator.generate_sharpness_consistency_analysis(
            processed_data,
            target_sharpness=TARGET_SHARPNESS,
            save_path=chart_dir / "sharpness_consistency.png"
        )
        
        # ç”Ÿæˆå¤„ç†åçš„ç»Ÿè®¡å›¾è¡¨
        self.chart_generator.generate_sharpness_statistics(
            processed_data,
            save_path=chart_dir / "processed_statistics.png"
        )
        
        self.logger.info(f"å¤„ç†åæ¸…æ™°åº¦åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {chart_dir}")
    
    def _generate_training_jsons(self) -> None:
        """ç”Ÿæˆè®­ç»ƒç”¨JSONæ–‡ä»¶"""
        try:
            # ä¸ºæ¯ä¸ªæ•°æ®é›†ç”ŸæˆJSONæ–‡ä»¶
            for dataset_name, output_dir in DATASET_OUTPUT_PATHS.items():
                if output_dir.exists():
                    self._generate_dataset_json(dataset_name, output_dir)
            
            # ç”Ÿæˆåˆå¹¶çš„JSONæ–‡ä»¶
            self._generate_master_jsons()
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè®­ç»ƒJSONæ–‡ä»¶å¤±è´¥: {e}")
    
    def _generate_dataset_json(self, dataset_name: str, output_dir: Path) -> None:
        """ä¸ºå•ä¸ªæ•°æ®é›†ç”ŸæˆJSONæ–‡ä»¶"""
        # æ”¶é›†å›¾åƒæ–‡ä»¶
        image_files = list(output_dir.glob("*.png"))
        
        if not image_files:
            self.logger.warning(f"æ•°æ®é›† {dataset_name} ä¸­æ²¡æœ‰å›¾åƒæ–‡ä»¶")
            return
        
        # ç”Ÿæˆæœ‰ç›‘ç£è®­ç»ƒJSON
        if GENERATE_TRAINING_JSON:
            # æŸ¥æ‰¾å¯¹åº”çš„æ©ç æ–‡ä»¶
            mask_dir = Path(f"datasets/{dataset_name}/masks_3class")
            if mask_dir.exists():
                self._generate_supervised_json(dataset_name, output_dir, mask_dir, image_files)
        
        # ç”ŸæˆSSLé¢„è®­ç»ƒJSON
        if GENERATE_SSL_JSON:
            self._generate_ssl_json(dataset_name, output_dir, image_files)
    
    def _generate_supervised_json(self, dataset_name: str, image_dir: Path, 
                                 mask_dir: Path, image_files: List[Path]) -> None:
        """ç”Ÿæˆæœ‰ç›‘ç£è®­ç»ƒJSON"""
        samples = []
        
        for img_file in image_files:
            # æŸ¥æ‰¾å¯¹åº”çš„æ©ç æ–‡ä»¶
            mask_file = mask_dir / img_file.name
            if mask_file.exists():
                sample = {
                    'dataset': dataset_name,
                    'frames': [str(img_file.relative_to(image_dir))],
                    'mask_file': str(mask_file.relative_to(mask_dir))
                }
                samples.append(sample)
        
        if samples:
            json_data = {
                'samples': samples,
                'dataset_name': dataset_name,
                'root_raw_image_dir': str(image_dir),
                'root_labeled_mask_dir': str(mask_dir),
                'description': f'{dataset_name} æ¸…æ™°åº¦å¹³å‡åŒ–åçš„æœ‰ç›‘ç£åˆ†å‰²è®­ç»ƒæ•°æ®',
                'num_samples': len(samples)
            }
            
            output_path = Path("json") / f"{dataset_name}_sharpness_averaged.json"
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"å·²ç”Ÿæˆæœ‰ç›‘ç£JSON: {output_path}ï¼Œå…±{len(samples)}å¼ å›¾ç‰‡")
    
    def _generate_ssl_json(self, dataset_name: str, image_dir: Path, 
                           image_files: List[Path]) -> None:
        """ç”ŸæˆSSLé¢„è®­ç»ƒJSON"""
        samples = []
        
        for img_file in image_files:
            sample = {
                'dataset': dataset_name,
                'image_file': str(img_file.relative_to(image_dir))
            }
            samples.append(sample)
        
        if samples:
            json_data = {
                'samples': samples,
                'dataset_name': dataset_name,
                'root_raw_image_dir': str(image_dir),
                'description': f'{dataset_name} æ¸…æ™°åº¦å¹³å‡åŒ–åçš„SSLé¢„è®­ç»ƒæ•°æ®',
                'num_samples': len(samples)
            }
            
            output_path = Path("json") / f"{dataset_name}_sharpness_averaged_ssl.json"
            output_path.parent.mkdir(exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"å·²ç”ŸæˆSSL JSON: {output_path}ï¼Œå…±{len(samples)}å¼ å›¾ç‰‡")
    
    def _generate_master_jsons(self) -> None:
        """ç”Ÿæˆåˆå¹¶çš„JSONæ–‡ä»¶"""
        try:
            self.logger.info("å¼€å§‹ç”Ÿæˆåˆå¹¶çš„master JSONæ–‡ä»¶...")
            
            # æ”¶é›†æ‰€æœ‰æ•°æ®é›†çš„æ ·æœ¬
            all_supervised_samples = []
            all_ssl_samples = []
            datasets_info = {}
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_supervised = 0
            total_ssl = 0
            
            for dataset_name, output_dir in DATASET_OUTPUT_PATHS.items():
                self.logger.info(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
                
                if not output_dir.exists():
                    self.logger.warning(f"æ•°æ®é›† {dataset_name} çš„è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
                    continue
                
                # æ”¶é›†æœ‰ç›‘ç£æ ·æœ¬
                mask_dir = Path(f"datasets/{dataset_name}/masks_3class")
                supervised_count = 0
                ssl_count = 0
                
                if mask_dir.exists():
                    image_files = list(output_dir.glob("*.png"))
                    for img_file in image_files:
                        mask_file = mask_dir / img_file.name
                        if mask_file.exists():
                            all_supervised_samples.append({
                                'dataset': dataset_name,
                                'frames': [str(img_file.relative_to(output_dir))],
                                'mask_file': str(mask_file.relative_to(mask_dir))
                            })
                            supervised_count += 1
                        else:
                            self.logger.debug(f"å›¾åƒ {img_file.name} æ²¡æœ‰å¯¹åº”çš„maskæ–‡ä»¶")
                
                # æ”¶é›†SSLæ ·æœ¬ï¼ˆæ‰€æœ‰å›¾åƒéƒ½å¯ä»¥ç”¨äºSSLï¼‰
                image_files = list(output_dir.glob("*.png"))
                for img_file in image_files:
                    all_ssl_samples.append({
                        'dataset': dataset_name,
                        'image_file': str(img_file.relative_to(output_dir))
                    })
                    ssl_count += 1
                
                # è®°å½•æ•°æ®é›†ä¿¡æ¯
                datasets_info[dataset_name] = {
                    'raw_image_root': str(output_dir),
                    'mask_root': str(mask_dir) if mask_dir.exists() else None,
                    'supervised_count': supervised_count,
                    'ssl_count': ssl_count,
                    'total_images': len(image_files)
                }
                
                total_supervised += supervised_count
                total_ssl += ssl_count
                
                self.logger.info(f"æ•°æ®é›† {dataset_name}: æœ‰ç›‘ç£æ ·æœ¬ {supervised_count}, SSLæ ·æœ¬ {ssl_count}, æ€»å›¾åƒ {len(image_files)}")
            
            # ç”Ÿæˆåˆå¹¶çš„æœ‰ç›‘ç£JSON
            if all_supervised_samples:
                master_supervised_json = Path("json") / "master_sharpness_averaged_dataset.json"
                master_supervised_json.parent.mkdir(exist_ok=True)
                
                supervised_data = {
                    'samples': all_supervised_samples,
                    'datasets_info': datasets_info,
                    'description': 'æ‰€æœ‰æ•°æ®é›†æ¸…æ™°åº¦å¹³å‡åŒ–åçš„æœ‰ç›‘ç£åˆ†å‰²è®­ç»ƒæ ·æœ¬',
                    'num_samples': len(all_supervised_samples),
                    'generation_time': datetime.now().isoformat(),
                    'total_supervised': total_supervised,
                    'total_ssl': total_ssl
                }
                
                with open(master_supervised_json, 'w', encoding='utf-8') as f:
                    json.dump(supervised_data, f, indent=2, ensure_ascii=False)
                
                self.logger.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶æœ‰ç›‘ç£JSON: {master_supervised_json}")
                self.logger.info(f"   åŒ…å« {len(all_supervised_samples)} ä¸ªæœ‰ç›‘ç£æ ·æœ¬")
                self.logger.info(f"   æ¥è‡ª {len(datasets_info)} ä¸ªæ•°æ®é›†")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰ç›‘ç£æ ·æœ¬ï¼Œè·³è¿‡ç”Ÿæˆæœ‰ç›‘ç£JSON")
            
            # ç”Ÿæˆåˆå¹¶çš„SSL JSON
            if all_ssl_samples:
                master_ssl_json = Path("json") / "master_sharpness_averaged_ssl_dataset.json"
                master_ssl_json.parent.mkdir(exist_ok=True)
                
                ssl_data = {
                    'samples': all_ssl_samples,
                    'datasets_info': {k: v['raw_image_root'] for k, v in datasets_info.items()},
                    'description': 'æ‰€æœ‰æ•°æ®é›†æ¸…æ™°åº¦å¹³å‡åŒ–åçš„SSLé¢„è®­ç»ƒæ ·æœ¬',
                    'num_samples': len(all_ssl_samples),
                    'generation_time': datetime.now().isoformat(),
                    'total_ssl': total_ssl
                }
                
                with open(master_ssl_json, 'w', encoding='utf-8') as f:
                    json.dump(ssl_data, f, indent=2, ensure_ascii=False)
                
                self.logger.info(f"âœ… å·²ç”Ÿæˆåˆå¹¶SSL JSON: {master_ssl_json}")
                self.logger.info(f"   åŒ…å« {len(all_ssl_samples)} ä¸ªSSLæ ·æœ¬")
                self.logger.info(f"   æ¥è‡ª {len(datasets_info)} ä¸ªæ•°æ®é›†")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°SSLæ ·æœ¬ï¼Œè·³è¿‡ç”ŸæˆSSL JSON")
            
            # ç”Ÿæˆå¤„ç†æ‘˜è¦
            summary = {
                'total_datasets': len(datasets_info),
                'total_supervised_samples': total_supervised,
                'total_ssl_samples': total_ssl,
                'datasets_breakdown': datasets_info,
                'generation_time': datetime.now().isoformat()
            }
            
            summary_json = Path("json") / "sharpness_processing_summary.json"
            with open(summary_json, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“Š å·²ç”Ÿæˆå¤„ç†æ‘˜è¦: {summary_json}")
            self.logger.info("ğŸ‰ Master JSONæ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
                
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆåˆå¹¶JSONæ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            raise
    
    def _validate_generated_jsons(self) -> bool:
        """éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶æ˜¯å¦æ­£ç¡®"""
        try:
            self.logger.info("å¼€å§‹éªŒè¯ç”Ÿæˆçš„JSONæ–‡ä»¶...")
            
            validation_results = {}
            
            # éªŒè¯æœ‰ç›‘ç£JSON
            supervised_json = Path("json") / "master_sharpness_averaged_dataset.json"
            if supervised_json.exists():
                with open(supervised_json, 'r', encoding='utf-8') as f:
                    supervised_data = json.load(f)
                
                # æ£€æŸ¥åŸºæœ¬ç»“æ„
                required_keys = ['samples', 'datasets_info', 'num_samples']
                structure_valid = all(key in supervised_data for key in required_keys)
                
                # æ£€æŸ¥æ ·æœ¬æ•°é‡
                samples_count = len(supervised_data.get('samples', []))
                declared_count = supervised_data.get('num_samples', 0)
                count_valid = samples_count == declared_count
                
                # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯
                datasets_valid = len(supervised_data.get('datasets_info', {})) > 0
                
                validation_results['supervised'] = {
                    'exists': True,
                    'structure_valid': structure_valid,
                    'count_valid': count_valid,
                    'datasets_valid': datasets_valid,
                    'sample_count': samples_count,
                    'declared_count': declared_count
                }
                
                self.logger.info(f"æœ‰ç›‘ç£JSONéªŒè¯: ç»“æ„{'âœ…' if structure_valid else 'âŒ'}, æ•°é‡{'âœ…' if count_valid else 'âŒ'}, æ•°æ®é›†{'âœ…' if datasets_valid else 'âŒ'}")
            else:
                validation_results['supervised'] = {'exists': False}
                self.logger.warning("æœ‰ç›‘ç£JSONæ–‡ä»¶ä¸å­˜åœ¨")
            
            # éªŒè¯SSL JSON
            ssl_json = Path("json") / "master_sharpness_averaged_ssl_dataset.json"
            if ssl_json.exists():
                with open(ssl_json, 'r', encoding='utf-8') as f:
                    ssl_data = json.load(f)
                
                # æ£€æŸ¥åŸºæœ¬ç»“æ„
                required_keys = ['samples', 'datasets_info', 'num_samples']
                structure_valid = all(key in ssl_data for key in required_keys)
                
                # æ£€æŸ¥æ ·æœ¬æ•°é‡
                samples_count = len(ssl_data.get('samples', []))
                declared_count = ssl_data.get('num_samples', 0)
                count_valid = samples_count == declared_count
                
                # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯
                datasets_valid = len(ssl_data.get('datasets_info', {})) > 0
                
                validation_results['ssl'] = {
                    'exists': True,
                    'structure_valid': structure_valid,
                    'count_valid': count_valid,
                    'datasets_valid': datasets_valid,
                    'sample_count': samples_count,
                    'declared_count': declared_count
                }
                
                self.logger.info(f"SSL JSONéªŒè¯: ç»“æ„{'âœ…' if structure_valid else 'âŒ'}, æ•°é‡{'âœ…' if count_valid else 'âŒ'}, æ•°æ®é›†{'âœ…' if datasets_valid else 'âŒ'}")
            else:
                validation_results['ssl'] = {'exists': False}
                self.logger.warning("SSL JSONæ–‡ä»¶ä¸å­˜åœ¨")
            
            # éªŒè¯æ‘˜è¦JSON
            summary_json = Path("json") / "sharpness_processing_summary.json"
            if summary_json.exists():
                with open(summary_json, 'r', encoding='utf-8') as f:
                    summary_data = json.load(f)
                
                # æ£€æŸ¥åŸºæœ¬ç»“æ„
                required_keys = ['total_datasets', 'total_supervised_samples', 'total_ssl_samples']
                structure_valid = all(key in summary_data for key in required_keys)
                
                validation_results['summary'] = {
                    'exists': True,
                    'structure_valid': structure_valid
                }
                
                self.logger.info(f"æ‘˜è¦JSONéªŒè¯: ç»“æ„{'âœ…' if structure_valid else 'âŒ'}")
            else:
                validation_results['summary'] = {'exists': False}
                self.logger.warning("æ‘˜è¦JSONæ–‡ä»¶ä¸å­˜åœ¨")
            
            # æ€»ä½“éªŒè¯ç»“æœ
            all_valid = all(
                result.get('exists', False) and 
                result.get('structure_valid', False) and 
                result.get('count_valid', True) and 
                result.get('datasets_valid', True)
                for result in validation_results.values()
                if result.get('exists', False)
            )
            
            if all_valid:
                self.logger.info("ğŸ‰ æ‰€æœ‰JSONæ–‡ä»¶éªŒè¯é€šè¿‡ï¼")
            else:
                self.logger.warning("âš ï¸ éƒ¨åˆ†JSONæ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
            
            return all_valid
            
        except Exception as e:
            self.logger.error(f"âŒ JSONæ–‡ä»¶éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_processing_summary(self) -> Dict:
        """è·å–å¤„ç†æ‘˜è¦"""
        summary = {
            "input_folder": str(INPUT_FOLDER),
            "output_folders": {name: str(path) for name, path in DATASET_OUTPUT_PATHS.items()},
            "total_processed": len(self.processed_images) if hasattr(self, 'processed_images') else 0,
            "chart_output": str(CHART_OUTPUT_FOLDER),
            "json_output": str(JSON_OUTPUT_PATH),
            "sharpness_averaging_enabled": ENABLE_SHARPNESS_AVERAGING,
            "averaging_method": AVERAGING_METHOD,
            "target_sharpness": TARGET_SHARPNESS
        }
        
        # ç»Ÿè®¡æ¯ä¸ªæ•°æ®é›†çš„å›¾åƒæ•°é‡
        for dataset_name, output_dir in DATASET_OUTPUT_PATHS.items():
            if output_dir.exists():
                image_count = len(list(output_dir.glob("*.png")))
                summary[f"{dataset_name}_image_count"] = image_count
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = ImageSharpnessProcessor()
        
        # å¤„ç†å›¾åƒ
        processor.process_images()
        
        # æ‰“å°æ‘˜è¦
        summary = processor.get_processing_summary()
        print("\n" + "="*50)
        print("å›¾åƒæ¸…æ™°åº¦å¹³å‡åŒ–å¤„ç†å®Œæˆï¼")
        print("="*50)
        print("å¤„ç†æ‘˜è¦:")
        for key, value in summary.items():
            if isinstance(value, dict):
                print(f"  {key}:")
                for sub_key, sub_value in value.items():
                    print(f"    {sub_key}: {sub_value}")
            else:
                print(f"  {key}: {value}")
        print("="*50)
        
    except Exception as e:
        print(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
