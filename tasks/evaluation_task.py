import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os
from tqdm import tqdm
from easydict import EasyDict as edict
from PIL import Image
from pathlib import Path
import json
from sklearn.metrics import jaccard_score

# å¯¼å…¥é…ç½®
import configs.base as base_config_module
from configs.inference.evaluation_config import get_evaluation_config
from datasets.sem_datasets import SemSegmentationDataset
from models.segmentation_unet import SegmentationUNet

# è¾…åŠ©å‡½æ•°
def build_base_config_from_module(module):
    """æ ¹æ® base.py çš„ç»“æ„æ„å»º EasyDict é…ç½®å¯¹è±¡ã€‚"""
    print("æ­£åœ¨ä» 'configs/base.py' åŠ è½½é…ç½®å˜é‡...")
    try:
        base_config = edict()
        base_config.IMAGE_SIZE = (module.IMAGE_HEIGHT, module.IMAGE_WIDTH)
        base_config.INPUT_DEPTH = module.INPUT_DEPTH
        base_config.NUM_CLASSES = module.NUM_CLASSES
        
        # å¤„ç†MAPPINGå­—å…¸ï¼Œå°†å…ƒç»„é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²é”®ä»¥é¿å…easydictçš„ç±»å‹é”™è¯¯
        raw_mapping = getattr(module, 'MAPPING', {})
        if raw_mapping:
            # å°†å…ƒç»„é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
            converted_mapping = {}
            original_mapping = {}
            for rgb_tuple, class_id in raw_mapping.items():
                if isinstance(rgb_tuple, tuple):
                    # å°†RGBå…ƒç»„è½¬æ¢ä¸ºå­—ç¬¦ä¸²é”®ï¼Œä¾‹å¦‚ "(0, 0, 255)" -> "0_0_255"
                    rgb_key = "_".join(map(str, rgb_tuple))
                    # åŒæ—¶ä¿å­˜è½¬æ¢åçš„åŸå§‹æ˜ å°„ï¼Œé¿å…easydictçš„å…ƒç»„é”®é—®é¢˜
                    original_mapping[rgb_key] = class_id
                else:
                    rgb_key = str(rgb_tuple)
                    original_mapping[rgb_key] = class_id
                converted_mapping[rgb_key] = class_id
            base_config.MAPPING = converted_mapping
            # ä¿å­˜è½¬æ¢åçš„åŸå§‹æ˜ å°„ç”¨äºå‘åå…¼å®¹
            base_config.ORIGINAL_MAPPING = original_mapping
        else:
            base_config.MAPPING = {}
            base_config.ORIGINAL_MAPPING = {}
        
        # å¤„ç†CLASS_NAMESå­—å…¸ï¼Œå°†æ•´æ•°é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²é”®ä»¥é¿å…easydictçš„ç±»å‹é”™è¯¯
        raw_class_names = getattr(module, 'CLASS_NAMES', {})
        if raw_class_names and isinstance(raw_class_names, dict):
            # å°†æ•´æ•°é”®è½¬æ¢ä¸ºå­—ç¬¦ä¸²é”®
            converted_class_names = {}
            for class_id, class_name in raw_class_names.items():
                converted_class_names[str(class_id)] = class_name
            base_config.CLASS_NAMES = converted_class_names
        else:
            # å¦‚æœæ²¡æœ‰CLASS_NAMESå­—å…¸ï¼Œä½¿ç”¨é»˜è®¤çš„åˆ—è¡¨æ ¼å¼
            base_config.CLASS_NAMES = ['class_%d' % i for i in range(base_config.NUM_CLASSES)]
        print("æˆåŠŸæ„å»ºåŸºç¡€é…ç½®å¯¹è±¡ã€‚")
        return base_config
    except AttributeError as e:
        print(f"é”™è¯¯: åœ¨ 'configs/base.py' ä¸­æ‰¾ä¸åˆ°å¿…è¦çš„é…ç½®å˜é‡ã€‚ç¼ºå¤±äº†: {e}")
        exit()

def get_device():
    """è·å–è®¡ç®—è®¾å¤‡ (GPU/CPU)"""
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    print(f"è®¾å¤‡æ£€æµ‹: {'CUDA (GPU)' if use_cuda else 'CPU'} å°†è¢«ä½¿ç”¨ã€‚")
    return device

def strip_mask_suffix(name):
    for suf in ['_mask', '-mask', '.mask']:
        if name.endswith(suf):
            return name[:-len(suf)]
    return name

def load_mask(mask_path, base_config):
    mask = np.array(Image.open(mask_path))
    if mask.ndim == 3:
        # ä»è½¬æ¢åçš„æ˜ å°„é‡å»ºRGBå…ƒç»„åˆ°ç±»åˆ«IDçš„æ˜ å°„
        mapping = {}
        for rgb_key, class_id in base_config.MAPPING.items():
            if '_' in rgb_key:
                # å°†å­—ç¬¦ä¸²é”®è½¬æ¢å›RGBå…ƒç»„
                rgb_values = tuple(map(int, rgb_key.split('_')))
                mapping[rgb_values] = class_id
            else:
                # å¤„ç†éå…ƒç»„é”®çš„æƒ…å†µ
                mapping[rgb_key] = class_id
        
        out = np.zeros(mask.shape[:2], dtype=np.int64)
        for rgb, idx in mapping.items():
            if isinstance(rgb, tuple):
                out[(mask == rgb).all(axis=-1)] = idx
            else:
                # å¤„ç†éå…ƒç»„é”®çš„æƒ…å†µ
                out[(mask == rgb)] = idx
        return out
    return mask

def dice_score(pred, gt, num_classes):
    """è®¡ç®—Diceç³»æ•°ï¼Œä¸è®­ç»ƒæ—¶çš„è®¡ç®—æ–¹å¼ä¿æŒä¸€è‡´"""
    scores = []
    for i in range(num_classes):
        pred_i = (pred == i)
        gt_i = (gt == i)
        intersection = (pred_i & gt_i).sum()
        union = pred_i.sum() + gt_i.sum()
        if union == 0:
            scores.append(1.0)
        else:
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å¹³æ»‘å› å­
            scores.append(2.0 * intersection / (union + 1e-6))
    return np.array(scores)

def iou_score(pred, gt, num_classes):
    """è®¡ç®—IoUç³»æ•°ï¼Œä¸è®­ç»ƒæ—¶çš„è®¡ç®—æ–¹å¼ä¿æŒä¸€è‡´"""
    scores = []
    for i in range(num_classes):
        pred_i = (pred == i)
        gt_i = (gt == i)
        intersection = (pred_i & gt_i).sum()
        union = (pred_i | gt_i).sum()
        if union == 0:
            scores.append(1.0)
        else:
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å¹³æ»‘å› å­
            scores.append(intersection / (union + 1e-6))
    return np.array(scores)

def calculate_psnr_ssim(pred_mask, gt_mask, num_classes):
    """è®¡ç®—PSNRå’ŒSSIMæŒ‡æ ‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰"""
    try:
        from skimage.metrics import peak_signal_noise_ratio as psnr
        from skimage.metrics import structural_similarity as ssim
        
        psnr_scores = []
        ssim_scores = []
        
        for i in range(num_classes):
            pred_i = (pred_mask == i).astype(np.float32)
            gt_i = (gt_mask == i).astype(np.float32)
            
            # è®¡ç®—PSNR
            try:
                data_range = gt_i.max() - gt_i.min()
                if data_range == 0:
                    data_range = 1.0
                psnr_val = psnr(gt_i, pred_i, data_range=data_range)
                if isinstance(psnr_val, tuple):
                    psnr_val = psnr_val[0]
                psnr_scores.append(float(psnr_val))
            except Exception as e:
                psnr_scores.append(0.0)
            
            # è®¡ç®—SSIM
            try:
                data_range = gt_i.max() - gt_i.min()
                if data_range == 0:
                    data_range = 1.0
                ssim_val = ssim(gt_i, pred_i, data_range=data_range)
                if isinstance(ssim_val, tuple):
                    ssim_val = ssim_val[0]
                ssim_scores.append(float(ssim_val))
            except Exception as e:
                ssim_scores.append(0.0)
        
        return np.array(psnr_scores), np.array(ssim_scores)
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£…skimageï¼Œè¿”å›é›¶å€¼
        return np.zeros(num_classes), np.zeros(num_classes)

# æ ¸å¿ƒå‡½æ•°
def evaluate_model(config, base_config):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("--- å¼€å§‹æ¨¡å‹è¯„ä¼°ä»»åŠ¡ ---")
    
    if config.EVALUATION_MODE == 'random_sample':
        print("--- éšæœºæŠ½å–è¯„ä¼°æ¨¡å¼ ---")
        
        # æ£€æŸ¥è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"è®¾å¤‡æ£€æµ‹: {'CUDA (GPU)' if device.type == 'cuda' else 'CPU'} å°†è¢«ä½¿ç”¨ã€‚")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ç»“æœå°†ä¿å­˜è‡³: {output_dir}")
        
        # å®šä¹‰è¦è¯„ä¼°çš„æ•°æ®é›†
        # æ³¨æ„ï¼šåªæœ‰masteræ–‡ä»¶åŒ…å«å®Œæ•´çš„datasets_infoï¼Œå…¶ä»–å•ç‹¬çš„æ–‡ä»¶éœ€è¦ç‰¹æ®Šå¤„ç†
        datasets_to_evaluate = [
            ('dataset1_LInCl', 'json/dataset1_LInCl.json'),
            ('dataset2_LPSCl', 'json/dataset2_LPSCl.json'),
            ('dataset3_LNOCl', 'json/dataset3_LNOCl.json'),
            ('master', 'json/master_sharpness_averaged_dataset.json')
        ]
        
        all_metrics = {}
        
        # è¯„ä¼°æ¯ä¸ªæ•°æ®é›†
        for dataset_name, json_file in datasets_to_evaluate:
            print(f"\n{'='*20} è¯„ä¼°æ•°æ®é›†: {dataset_name} {'='*20}")
            
            # æ£€æŸ¥æ•°æ®é›†ä¸€è‡´æ€§
            check_dataset_consistency(dataset_name, json_file)
            
            # ä¸´æ—¶ä¿®æ”¹é…ç½®ä»¥ä½¿ç”¨å½“å‰æ•°æ®é›†
            original_json = config.DATA_LOADER_CONFIG.val.json_file_identifier
            config.DATA_LOADER_CONFIG.val.json_file_identifier = json_file
            
            try:
                # è¯„ä¼°éªŒè¯é›†
                print(f"\n--- è¯„ä¼° {dataset_name} éªŒè¯é›† ===")
                val_metrics = evaluate_random_samples(config, base_config, split='val')
                
                # å¦‚æœé…ç½®äº†åŒæ—¶è¯„ä¼°è®­ç»ƒé›†
                if hasattr(config, 'EVALUATE_BOTH_SPLITS') and config.EVALUATE_BOTH_SPLITS:
                    print(f"\n--- è¯„ä¼° {dataset_name} è®­ç»ƒé›† ===")
                    train_metrics = evaluate_random_samples(config, base_config, split='train')
                    
                    # åˆå¹¶ç»“æœ
                    all_metrics[dataset_name] = {
                        'val': val_metrics,
                        'train': train_metrics
                    }
                    
                    print(f"\n=== {dataset_name} ç»¼åˆè¯„ä¼°ç»“æœ ===")
                    print("éªŒè¯é›†æ€§èƒ½:")
                    print_metrics_summary(val_metrics)
                    print("\nè®­ç»ƒé›†æ€§èƒ½:")
                    print_metrics_summary(train_metrics)
                else:
                    all_metrics[dataset_name] = val_metrics
                    print(f"\n=== {dataset_name} éªŒè¯é›†è¯„ä¼°ç»“æœ ===")
                    print_metrics_summary(val_metrics)
                    
            except Exception as e:
                print(f"è¯„ä¼°æ•°æ®é›† {dataset_name} æ—¶å‡ºé”™: {e}")
                continue
            finally:
                # æ¢å¤åŸå§‹é…ç½®
                config.DATA_LOADER_CONFIG.val.json_file_identifier = original_json
        
        # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®é›†çš„ç»¼åˆå¯¹æ¯”
        if len(all_metrics) > 1:
            print("\n" + "="*60)
            print("=== æ‰€æœ‰æ•°æ®é›†æ€§èƒ½å¯¹æ¯” ===")
            print("="*60)
            print_overall_metrics(all_metrics)
            
            # åˆ†æè®­ç»ƒå’Œè¯„ä¼°ç»“æœçš„å·®å¼‚
            analyze_training_evaluation_discrepancy()
    
    elif config.EVALUATION_MODE == 'inference_result':
        print("--- æ¨ç†ç»“æœè¯„ä¼°æ¨¡å¼ ---")
        evaluate_inference_results(config, base_config)
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¯„ä¼°æ¨¡å¼: {config.EVALUATION_MODE}")
    
    print("--- æ¨¡å‹è¯„ä¼°ä»»åŠ¡å®Œæˆ ---")

def evaluate_random_samples(config, base_config, split='val'):
    """æ¨¡å¼1: éšæœºæŠ½å–æ•°æ®é›†ä¸­çš„å›¾ç‰‡è¿›è¡Œè¯„ä¼°"""
    print(f"--- éšæœºæŠ½å– {split} é›†è¯„ä¼°æ¨¡å¼ ---")
    
    device = get_device()
    output_dir = config.OUTPUT_DIR
    os.makedirs(output_dir, exist_ok=True)
    print(f"ç»“æœå°†ä¿å­˜è‡³: {output_dir}")
    
    print(f"åŠ è½½ {split} æ•°æ®é›†...")
    
    # æ£€æŸ¥JSONæ–‡ä»¶æ˜¯å¦åŒ…å«datasets_infoï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ é»˜è®¤è·¯å¾„
    json_path = Path(__file__).resolve().parent.parent / config.DATA_LOADER_CONFIG.val.json_file_identifier
    if json_path.exists():
        with open(json_path, 'r') as f:
            data_info = json.load(f)
        
        # å¦‚æœç¼ºå°‘datasets_infoï¼Œæ·»åŠ é»˜è®¤è·¯å¾„
        if 'datasets_info' not in data_info:
            print("æ£€æµ‹åˆ°ç¼ºå°‘datasets_infoå­—æ®µï¼Œæ·»åŠ é»˜è®¤è·¯å¾„...")
            # ä»JSONæ–‡ä»¶åæ¨æ–­æ•°æ®é›†åç§°
            json_filename = Path(config.DATA_LOADER_CONFIG.val.json_file_identifier).stem
            if 'dataset1_LInCl' in json_filename:
                data_info['datasets_info'] = {
                    'dataset1_LInCl': {
                        'raw_image_root': 'datasets/dataset1_LInCl/raw_images',
                        'mask_root': 'datasets/dataset1_LInCl/masks_3class'
                    }
                }
            elif 'dataset2_LPSCl' in json_filename:
                data_info['datasets_info'] = {
                    'dataset2_LPSCl': {
                        'raw_image_root': 'datasets/dataset2_LPSCl/raw_images',
                        'mask_root': 'datasets/dataset2_LPSCl/masks_3class'
                    }
                }
            elif 'dataset3_LNOCl' in json_filename:
                data_info['datasets_info'] = {
                    'dataset3_LNOCl': {
                        'raw_image_root': 'datasets/dataset3_LNOCl/raw_images',
                        'mask_root': 'datasets/dataset3_LNOCl/masks_3class'
                    }
                }
            
            # å°†ä¿®æ”¹åçš„æ•°æ®å†™å›JSONæ–‡ä»¶ï¼ˆä¸´æ—¶ï¼‰
            temp_json_path = json_path.parent / f"temp_{json_path.name}"
            with open(temp_json_path, 'w') as f:
                json.dump(data_info, f, indent=2)
            
            # ä¸´æ—¶ä½¿ç”¨ä¿®æ”¹åçš„JSONæ–‡ä»¶
            temp_json_identifier = f"temp_{Path(config.DATA_LOADER_CONFIG.val.json_file_identifier).name}"
            val_dataset = SemSegmentationDataset(
                json_file_identifier=temp_json_identifier,
                project_root=Path(__file__).resolve().parent.parent,
                split=split
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_json_path.unlink()
        else:
            val_dataset = SemSegmentationDataset(
                json_file_identifier=config.DATA_LOADER_CONFIG.val.json_file_identifier,
                project_root=Path(__file__).resolve().parent.parent,
                split=split
            )
    else:
        val_dataset = SemSegmentationDataset(
            json_file_identifier=config.DATA_LOADER_CONFIG.val.json_file_identifier,
            project_root=Path(__file__).resolve().parent.parent,
            split=split
        )
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=config.DATA_LOADER_CONFIG.val.num_workers)
    
    print("åŠ è½½æ¨¡å‹...")
    
    # åˆ›å»ºç›‘ç£å­¦ä¹ æ¨¡å‹
    from models.segmentation_unet import SegmentationUNet
    model = SegmentationUNet(
        encoder_name=config.MODEL_CONFIG.encoder_name,
        decoder_name=config.MODEL_CONFIG.decoder_name,
        n_channels=base_config.INPUT_DEPTH,
        n_classes=base_config.NUM_CLASSES
    )
    print(f"åˆ›å»ºç›‘ç£å­¦ä¹ æ¨¡å‹: {config.MODEL_CONFIG.model_class}")
    
    checkpoint_path = config.CHECKPOINT_PATH
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶: {checkpoint_path}")
    
    print(f"ä» {checkpoint_path} åŠ è½½æƒé‡...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†æƒé‡åŠ è½½
    if config.MODEL_TYPE == 'supervised':
        # ç›‘ç£å­¦ä¹ æ¨¡å‹æƒé‡åŠ è½½
        if 'model_state_dict' in checkpoint:
            # è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ£€æŸ¥ç‚¹ï¼Œéœ€è¦æå–æ¨¡å‹æƒé‡
            state_dict = checkpoint['model_state_dict']
            print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æƒé‡ï¼Œè®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'unknown')}")
        else:
            # è¿™æ˜¯ç›´æ¥çš„æ¨¡å‹æƒé‡æ–‡ä»¶
            state_dict = checkpoint
            print("ç›´æ¥åŠ è½½æ¨¡å‹æƒé‡æ–‡ä»¶")
        
        model.load_state_dict(state_dict)
        
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model = model.to(device)
    model.eval()
    
    num_to_inspect = config.NUM_IMAGES_TO_INSPECT if config.NUM_IMAGES_TO_INSPECT > 0 else len(val_loader)
    
    dice_all = []
    iou_all = []
    lines = []
    
    with torch.no_grad():
        for i, (images, true_masks) in enumerate(tqdm(val_loader, total=num_to_inspect, desc="è¯„ä¼°è¿›åº¦")):
            if i >= num_to_inspect: break
            
            images = images.to(device, dtype=torch.float32)
            
            # ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼šåˆ†å‰²ä»»åŠ¡
            pred_logits = model(images)
            # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„é¢„æµ‹æ–¹å¼ï¼šargmaxè€Œä¸æ˜¯é˜ˆå€¼åŒ–
            pred_masks = torch.argmax(pred_logits, dim=1)
            
            image_np = images.cpu().numpy().squeeze()
            true_mask_np = true_masks.cpu().numpy().squeeze()
            pred_mask_np = pred_masks.cpu().numpy().squeeze()

            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯æ­£ç¡®çš„ç»´åº¦ç”¨äºå¯è§†åŒ–
            # å›¾åƒåº”è¯¥æ˜¯ (height, width) æˆ– (height, width, channels)
            # æ©ç åº”è¯¥æ˜¯ (height, width)
            
            # å¤„ç†å›¾åƒæ•°æ®
            if image_np.ndim == 3:
                if image_np.shape[0] == 3:  # (channels, height, width)
                    image_np = image_np.transpose(1, 2, 0)  # è½¬æ¢ä¸º (height, width, channels)
                elif image_np.shape[2] == 3:  # (height, width, channels)
                    pass  # å·²ç»æ˜¯æ­£ç¡®çš„æ ¼å¼
                else:
                    # å–ä¸­é—´é€šé“
                    s = image_np.shape[0] // 2
                    image_np = image_np[s]
            
            # å¤„ç†çœŸå®æ©ç æ•°æ®
            if true_mask_np.ndim == 3:
                if true_mask_np.shape[0] == base_config.NUM_CLASSES:  # (classes, height, width)
                    # å–ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºä¸»è¦æ©ç 
                    true_mask_np = true_mask_np[0]
                elif true_mask_np.shape[2] == base_config.NUM_CLASSES:  # (height, width, classes)
                    # å–ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºä¸»è¦æ©ç 
                    true_mask_np = true_mask_np[:, :, 0]
                else:
                    # å–ä¸­é—´é€šé“
                    s = true_mask_np.shape[0] // 2
                    true_mask_np = true_mask_np[s]
            
            # å¤„ç†é¢„æµ‹æ©ç æ•°æ®
            if pred_mask_np.ndim == 3:
                if pred_mask_np.shape[0] == base_config.NUM_CLASSES:  # (classes, height, width)
                    # å–ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºä¸»è¦æ©ç 
                    pred_mask_np = pred_mask_np[0]
                elif pred_mask_np.shape[2] == base_config.NUM_CLASSES:  # (height, width, classes)
                    # å–ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºä¸»è¦æ©ç 
                    pred_mask_np = pred_mask_np[:, :, 0]
                else:
                    # å–ä¸­é—´é€šé“
                    s = pred_mask_np.shape[0] // 2
                    pred_mask_np = pred_mask_np[s]
            
            # å¯è§†åŒ–
            if config.VISUALIZATION.save_images:
                fig, axes = plt.subplots(1, 3, figsize=config.VISUALIZATION.figure_size)
                fig.suptitle(f'Sample {i+1}', fontsize=16)
                axes[0].imshow(image_np, cmap='gray'); axes[0].set_title('Original Image'); axes[0].axis('off')
                axes[1].imshow(true_mask_np, cmap='gray'); axes[1].set_title('Ground Truth Mask'); axes[1].axis('off')
                axes[2].imshow(pred_mask_np, cmap='gray'); axes[2].set_title('Model Prediction'); axes[2].axis('off')
                plt.savefig(os.path.join(output_dir, f'evaluation_sample_{i+1:04d}.{config.VISUALIZATION.image_format}'), 
                           bbox_inches='tight', dpi=config.VISUALIZATION.dpi)
                plt.close(fig)
            
            # æŒ‡æ ‡è®¡ç®— - ä¸è®­ç»ƒæ—¶çš„è¯„ä¼°æ–¹å¼ä¿æŒä¸€è‡´
            if pred_mask_np.shape != true_mask_np.shape:
                pred_mask_np = np.array(Image.fromarray(pred_mask_np.astype(np.uint8)).resize(true_mask_np.shape[::-1], resample=Image.NEAREST))
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            pred_mask_np = pred_mask_np.astype(np.int64)
            true_mask_np = true_mask_np.astype(np.int64)
            
            # è®¡ç®—ä¸»è¦æŒ‡æ ‡
            dice = dice_score(pred_mask_np, true_mask_np, base_config.NUM_CLASSES)
            iou = iou_score(pred_mask_np, true_mask_np, base_config.NUM_CLASSES)
            
            # è®¡ç®—PSNRå’ŒSSIMï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            psnr_scores, ssim_scores = calculate_psnr_ssim(pred_mask_np, true_mask_np, base_config.NUM_CLASSES)
            
            dice_all.append(dice)
            iou_all.append(iou)
            
            # æ‰“å°è¯¦ç»†çš„æ¯æ ·æœ¬æŒ‡æ ‡
            line = f"Sample {i+1}: "
            for j in range(base_config.NUM_CLASSES):
                cname = base_config.CLASS_NAMES.get(str(j), f'class_{j}')
                line += f" {cname} Dice: {dice[j]:.4f} IoU: {iou[j]:.4f}"
                if psnr_scores[j] > 0:  # åªåœ¨æœ‰æ•ˆæ—¶æ˜¾ç¤ºPSNR/SSIM
                    line += f" PSNR: {psnr_scores[j]:.2f}dB SSIM: {ssim_scores[j]:.4f}"
            print(line)
            lines.append(line)
            
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„åƒç´ ç»Ÿè®¡
            for j in range(base_config.NUM_CLASSES):
                cname = base_config.CLASS_NAMES.get(str(j), f'class_{j}')
                pred_pixels = (pred_mask_np == j).sum()
                gt_pixels = (true_mask_np == j).sum()
                intersection_pixels = ((pred_mask_np == j) & (true_mask_np == j)).sum()
                
                # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„Diceå’ŒIoU
                if gt_pixels + pred_pixels > 0:
                    class_dice = 2.0 * intersection_pixels / (gt_pixels + pred_pixels + 1e-6)
                else:
                    class_dice = 1.0 if gt_pixels == 0 and pred_pixels == 0 else 0.0
                
                if gt_pixels + pred_pixels - intersection_pixels > 0:
                    class_iou = intersection_pixels / (gt_pixels + pred_pixels - intersection_pixels + 1e-6)
                else:
                    class_iou = 1.0 if gt_pixels == 0 and pred_pixels == 0 else 0.0
                
                print(f"    {cname}: é¢„æµ‹åƒç´ ={pred_pixels}, GTåƒç´ ={gt_pixels}, äº¤é›†={intersection_pixels}")
                print(f"         {cname} Dice: {class_dice:.4f}, IoU: {class_iou:.4f}")
            
            # æ·»åŠ æ ·æœ¬çº§åˆ«çš„è¯¦ç»†åˆ†æ
            print(f"    æ ·æœ¬ {i+1} æ€»ä½“ç»Ÿè®¡:")
            print(f"      å›¾åƒå½¢çŠ¶: {image_np.shape}, æ•°æ®ç±»å‹: {image_np.dtype}")
            print(f"      GTæ©ç å½¢çŠ¶: {true_mask_np.shape}, æ•°æ®ç±»å‹: {true_mask_np.dtype}")
            print(f"      é¢„æµ‹æ©ç å½¢çŠ¶: {pred_mask_np.shape}, æ•°æ®ç±»å‹: {pred_mask_np.dtype}")
            print(f"      GTæ©ç å€¼èŒƒå›´: [{true_mask_np.min()}, {true_mask_np.max()}]")
            print(f"      é¢„æµ‹æ©ç å€¼èŒƒå›´: [{pred_mask_np.min()}, {pred_mask_np.max()}]")
            print(f"      GTæ©ç å”¯ä¸€å€¼: {np.unique(true_mask_np)}")
            print(f"      é¢„æµ‹æ©ç å”¯ä¸€å€¼: {np.unique(pred_mask_np)}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹æ±‡æ€»ä¸åŒçš„æŒ‡æ ‡
    if config.MODEL_TYPE == 'supervised':
        # ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼šæ±‡æ€»åˆ†å‰²æŒ‡æ ‡
        dice_all = np.array(dice_all)
        iou_all = np.array(iou_all)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡æŒ‡æ ‡
        lines.append("\n=== æ€»ä½“å‡å€¼ ===")
        print("\n=== æ€»ä½“å‡å€¼ ===")
        
        for j in range(base_config.NUM_CLASSES):
            cname = base_config.CLASS_NAMES.get(str(j), f'class_{j}')
            dice_mean = dice_all[:,j].mean()
            iou_mean = iou_all[:,j].mean()
            
            # è®¡ç®—æ ‡å‡†å·®
            dice_std = dice_all[:,j].std()
            iou_std = iou_all[:,j].std()
            
            s = f"{cname} Dice: {dice_mean:.4f}Â±{dice_std:.4f} IoU: {iou_mean:.4f}Â±{iou_std:.4f}"
            print(s)
            lines.append(s)
        
        # è®¡ç®—æ€»ä½“å¹³å‡
        overall_dice = dice_all.mean()
        overall_iou = iou_all.mean()
        overall_dice_std = dice_all.std()
        overall_iou_std = iou_all.std()
        
        print(f"\n=== ç»¼åˆæŒ‡æ ‡ ===")
        print(f"å¹³å‡Dice: {overall_dice:.4f}Â±{overall_dice_std:.4f}")
        print(f"å¹³å‡IoU: {overall_iou:.4f}Â±{overall_iou_std:.4f}")
        
        lines.append(f"\n=== ç»¼åˆæŒ‡æ ‡ ===")
        lines.append(f"å¹³å‡Dice: {overall_dice:.4f}Â±{overall_dice_std:.4f}")
        lines.append(f"å¹³å‡IoU: {overall_iou:.4f}Â±{overall_iou_std:.4f}")
        
        # è¿”å›è¯¦ç»†çš„æŒ‡æ ‡ä¿¡æ¯
        metrics_summary = {
            'Dice': dice_all,
            'IoU': iou_all,
            'ClassNames': base_config.CLASS_NAMES,
            'OverallDice': overall_dice,
            'OverallIoU': overall_iou,
            'DiceStd': overall_dice_std,
            'IoUStd': overall_iou_std
        }
        

    
    # ä¿å­˜ç»“æœ
    result_save_path = config.RESULT_SAVE_PATH or os.path.join(output_dir, 'evaluation_results.txt')
    
    # ç¡®ä¿ç»“æœæ–‡ä»¶çš„ç›®å½•å­˜åœ¨
    result_dir = os.path.dirname(result_save_path)
    if result_dir:
        os.makedirs(result_dir, exist_ok=True)
    
    with open(result_save_path, 'w', encoding='utf-8') as f:
        for l in lines:
            f.write(l + '\n')
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_save_path}")
    
    print(f"\nè¯„ä¼°å®Œæˆï¼ç”Ÿæˆ {num_to_inspect} å¼ å¯¹æ¯”å›¾å’ŒæŒ‡æ ‡æŠ¥å‘Šã€‚")
    return metrics_summary

def evaluate_inference_results(config, base_config):
    """æ¨¡å¼2: è¯„ä¼°æ¨ç†é¢„æµ‹å‡ºçš„å›¾åƒ"""
    print("--- æ¨ç†ç»“æœè¯„ä¼°æ¨¡å¼ ---")
    
    output_dir = config.OUTPUT_DIR
    os.makedirs(output_dir, exist_ok=True)
    print(f"ç»“æœå°†ä¿å­˜è‡³: {output_dir}")
    
    # è·å–æ¨ç†ç»“æœæ–‡ä»¶åˆ—è¡¨
    pred_mask_dir = Path(config.PRED_MASK_DIR)
    if not pred_mask_dir.exists():
        raise FileNotFoundError(f"æ¨ç†ç»“æœç›®å½•ä¸å­˜åœ¨: {pred_mask_dir}")
    
    pred_files = list(pred_mask_dir.glob(f"*{config.PRED_MASK_EXTENSION}"))
    if not pred_files:
        raise FileNotFoundError(f"åœ¨ {pred_mask_dir} ä¸­æœªæ‰¾åˆ°æ¨ç†ç»“æœæ–‡ä»¶")
    
    print(f"æ‰¾åˆ° {len(pred_files)} ä¸ªæ¨ç†ç»“æœæ–‡ä»¶")
    
    # é™åˆ¶è¯„ä¼°æ•°é‡
    if config.MAX_EVALUATION_IMAGES > 0:
        pred_files = pred_files[:config.MAX_EVALUATION_IMAGES]
        print(f"å°†è¯„ä¼°å‰ {len(pred_files)} ä¸ªæ–‡ä»¶")
    
    dice_all = []
    iou_all = []
    lines = []
    evaluated_count = 0
    
    for pred_file in tqdm(pred_files, desc="è¯„ä¼°æ¨ç†ç»“æœ"):
        try:
            # åŠ è½½é¢„æµ‹æ©ç 
            pred_mask = np.array(Image.open(pred_file))
            
            # æŸ¥æ‰¾å¯¹åº”çš„GTæ©ç 
            gt_mask = find_gt_mask(pred_file, config, base_config)
            if gt_mask is None:
                if config.ONLY_EVALUATE_WITH_GT:
                    print(f"è·³è¿‡ {pred_file.name}: æœªæ‰¾åˆ°å¯¹åº”GT")
                    continue
                else:
                    print(f"è­¦å‘Š: {pred_file.name} æœªæ‰¾åˆ°å¯¹åº”GTï¼Œå°†è·³è¿‡æŒ‡æ ‡è®¡ç®—")
                    continue
            
            # æŒ‡æ ‡è®¡ç®—
            dice = dice_score(pred_mask, gt_mask, base_config.NUM_CLASSES)
            iou = iou_score(pred_mask, gt_mask, base_config.NUM_CLASSES)
            dice_all.append(dice)
            iou_all.append(dice)
            
            # å¯è§†åŒ–
            if config.VISUALIZATION.save_images:
                fig, axes = plt.subplots(1, 2, figsize=(12, 6))
                fig.suptitle(f'Inference Result: {pred_file.stem}', fontsize=16)
                axes[0].imshow(pred_mask, cmap='gray'); axes[0].set_title('Predicted Mask'); axes[0].axis('off')
                axes[1].imshow(gt_mask, cmap='gray'); axes[1].set_title('Ground Truth'); axes[1].axis('off')
                plt.savefig(os.path.join(output_dir, f'inference_eval_{pred_file.stem}.{config.VISUALIZATION.image_format}'), 
                           bbox_inches='tight', dpi=config.VISUALIZATION.dpi)
                plt.close(fig)
            
            line = f"{pred_file.name}: "
            for j in range(base_config.NUM_CLASSES):
                cname = base_config.CLASS_NAMES.get(str(j), f'class_{j}')
                line += f" {cname} Dice: {dice[j]:.4f} IoU: {iou[j]:.4f}"
            print(line)
            lines.append(line)
            
            evaluated_count += 1
            
        except Exception as e:
            print(f"å¤„ç† {pred_file.name} æ—¶å‡ºé”™: {e}")
            continue
    
    if evaluated_count == 0:
        print("æ²¡æœ‰æˆåŠŸè¯„ä¼°ä»»ä½•æ–‡ä»¶ï¼")
        return
    
    # æ±‡æ€»æŒ‡æ ‡
    dice_all = np.array(dice_all)
    iou_all = np.array(iou_all)
    lines.append("\n=== æ€»ä½“å‡å€¼ ===")
    for j in range(base_config.NUM_CLASSES):
        cname = base_config.CLASS_NAMES.get(str(j), f'class_{j}')
        s = f"{cname} Dice: {dice_all[:,j].mean():.4f} IoU: {iou_all[:,j].mean():.4f}"
        print(s)
        lines.append(s)
    print(f"å¹³å‡Dice: {dice_all.mean():.4f}")
    print(f"å¹³å‡IoU: {iou_all.mean():.4f}")
    lines.append(f"å¹³å‡Dice: {dice_all.mean():.4f}")
    lines.append(f"å¹³å‡IoU: {iou_all.mean():.4f}")
    
    # ä¿å­˜ç»“æœ
    result_save_path = config.RESULT_SAVE_PATH or os.path.join(output_dir, 'inference_evaluation_results.txt')
    with open(result_save_path, 'w', encoding='utf-8') as f:
        for l in lines:
            f.write(l + '\n')
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_save_path}")
    
    print(f"\næ¨ç†ç»“æœè¯„ä¼°å®Œæˆï¼æˆåŠŸè¯„ä¼° {evaluated_count} ä¸ªæ–‡ä»¶ã€‚")

def find_gt_mask(pred_file, config, base_config):
    """æŸ¥æ‰¾å¯¹åº”çš„GTæ©ç """
    # é¦–å…ˆå°è¯•ä»GTç›®å½•ç›´æ¥æŸ¥æ‰¾
    if hasattr(config, 'GT_MASK_DIR') and config.GT_MASK_DIR:
        gt_dir = Path(config.GT_MASK_DIR)
        # å°è¯•ä¸åŒçš„GTæ–‡ä»¶åæ¨¡å¼
        gt_patterns = [
            pred_file.stem.replace('_mask', '') + '.png',  # ç§»é™¤_maskåç¼€
            pred_file.stem.replace('_mask', '') + '_mask.png',  # ä¿æŒ_maskåç¼€
            pred_file.stem + '.png',  # å®Œå…¨åŒ¹é…
        ]
        
        for pattern in gt_patterns:
            gt_file = gt_dir / pattern
            if gt_file.exists():
                return np.array(Image.open(gt_file))
    
    # ä»JSONæ–‡ä»¶æŸ¥æ‰¾
    if hasattr(config, 'GT_JSON_PATHS') and config.GT_JSON_PATHS:
        # è¿™é‡Œå¯ä»¥å®ç°ä»JSONæ–‡ä»¶æŸ¥æ‰¾GTçš„é€»è¾‘
        # ç”±äºå®ç°è¾ƒå¤æ‚ï¼Œæš‚æ—¶è¿”å›None
        pass
    
    return None

def print_metrics_summary(metrics_dict):
    """æ‰“å°å•ä¸ªæ•°æ®é›†çš„æŒ‡æ ‡æ‘˜è¦"""
    print(f"\n--- æŒ‡æ ‡æ‘˜è¦ ---")
    

    
    # ç›‘ç£å­¦ä¹ æ¨¡å‹ï¼šæ˜¾ç¤ºåˆ†å‰²æŒ‡æ ‡
    if 'ClassNames' in metrics_dict:
        for j in range(len(metrics_dict['ClassNames'])):
            cname = metrics_dict['ClassNames'].get(str(j), f'class_{j}')
            dice_mean = metrics_dict['Dice'][:, j].mean()
            iou_mean = metrics_dict['IoU'][:, j].mean()
            dice_std = metrics_dict['Dice'][:, j].std()
            iou_std = metrics_dict['IoU'][:, j].std()
            s = f"{cname} Dice: {dice_mean:.4f}Â±{dice_std:.4f} IoU: {iou_mean:.4f}Â±{iou_std:.4f}"
            print(s)
        
        # ä½¿ç”¨æ–°çš„æŒ‡æ ‡ç»“æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'OverallDice' in metrics_dict:
            overall_dice = metrics_dict['OverallDice']
            overall_iou = metrics_dict['OverallIoU']
            overall_dice_std = metrics_dict.get('DiceStd', 0.0)
            overall_iou_std = metrics_dict.get('IoUStd', 0.0)
            print(f"å¹³å‡Dice: {overall_dice:.4f}Â±{overall_dice_std:.4f}")
            print(f"å¹³å‡IoU: {overall_iou:.4f}Â±{overall_iou_std:.4f}")
        else:
            # å‘åå…¼å®¹
            print(f"å¹³å‡Dice: {metrics_dict['Dice'].mean():.4f}")
            print(f"å¹³å‡IoU: {metrics_dict['IoU'].mean():.4f}")
    else:
        print("è­¦å‘Š: æ— æ³•è¯†åˆ«çš„æŒ‡æ ‡ç»“æ„")

def analyze_training_evaluation_discrepancy():
    """åˆ†æè®­ç»ƒå’Œè¯„ä¼°ç»“æœçš„å·®å¼‚"""
    print("\n" + "="*80)
    print("=== è®­ç»ƒ vs è¯„ä¼°ç»“æœå·®å¼‚åˆ†æ ===")
    print("="*80)
    
    print("\nğŸ” **å¯èƒ½çš„åŸå› åˆ†æï¼š**")
    print("1. **æ•°æ®åˆ†å¸ƒå·®å¼‚**ï¼šè®­ç»ƒæ—¶çš„éªŒè¯é›†ä¸è¯„ä¼°æ—¶çš„éªŒè¯é›†å¯èƒ½ä¸å®Œå…¨ä¸€è‡´")
    print("2. **æ•°æ®é¢„å¤„ç†å·®å¼‚**ï¼šè®­ç»ƒæ—¶å¯èƒ½æœ‰æ•°æ®å¢å¼ºï¼Œè¯„ä¼°æ—¶æ²¡æœ‰")
    print("3. **æ¨¡å‹çŠ¶æ€å·®å¼‚**ï¼šè®­ç»ƒæ—¶æ¨¡å‹å¯èƒ½å¤„äºä¸åŒçŠ¶æ€ï¼ˆå¦‚dropoutæ¿€æ´»ï¼‰")
    print("4. **ç±»åˆ«ä¸å¹³è¡¡**ï¼šæŸäº›ç±»åˆ«æ ·æœ¬è¾ƒå°‘ï¼Œå¯¼è‡´è¯„ä¼°ä¸ç¨³å®š")
    print("5. **æ ·æœ¬é€‰æ‹©å·®å¼‚**ï¼šè¯„ä¼°æ—¶å¯èƒ½é€‰æ‹©äº†æ›´å®¹æ˜“é¢„æµ‹çš„æ ·æœ¬")
    
    print("\nğŸ“Š **å»ºè®®çš„è°ƒè¯•æ­¥éª¤ï¼š**")
    print("1. æ£€æŸ¥è®­ç»ƒå’Œè¯„ä¼°æ—¶ä½¿ç”¨çš„éªŒè¯é›†æ˜¯å¦å®Œå…¨ä¸€è‡´")
    print("2. æ¯”è¾ƒè®­ç»ƒå’Œè¯„ä¼°æ—¶çš„æ•°æ®é¢„å¤„ç†æµç¨‹")
    print("3. åˆ†ææ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡å’Œåˆ†å¸ƒ")
    print("4. æ£€æŸ¥æ¨¡å‹åœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶çš„çŠ¶æ€è®¾ç½®")
    print("5. å¯¹æ¯”è®­ç»ƒæ—¥å¿—ä¸­çš„è¯¦ç»†æŒ‡æ ‡")
    
    print("\n" + "="*80)

def check_dataset_consistency(dataset_name, json_file):
    """æ£€æŸ¥æ•°æ®é›†çš„ä¸€è‡´æ€§"""
    print(f"\nğŸ” æ£€æŸ¥æ•°æ®é›† {dataset_name} çš„ä¸€è‡´æ€§...")
    
    try:
        json_path = Path(__file__).resolve().parent.parent / json_file
        if json_path.exists():
            with open(json_path, 'r') as f:
                data_info = json.load(f)
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            total_samples = len(data_info.get('samples', []))
            val_samples = len([s for s in data_info.get('samples', []) if s.get('split') == 'val'])
            train_samples = len([s for s in data_info.get('samples', []) if s.get('split') == 'train'])
            
            print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
            print(f"  éªŒè¯é›†æ ·æœ¬æ•°: {val_samples}")
            print(f"  è®­ç»ƒé›†æ ·æœ¬æ•°: {train_samples}")
            
            # æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
            if 'samples' in data_info:
                class_counts = {}
                for sample in data_info['samples']:
                    dataset = sample.get('dataset', 'unknown')
                    if dataset not in class_counts:
                        class_counts[dataset] = 0
                    class_counts[dataset] += 1
                
                print(f"  æ•°æ®é›†åˆ†å¸ƒ: {class_counts}")
            
            return True
        else:
            print(f"  âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            return False
    except Exception as e:
        print(f"  âŒ æ£€æŸ¥æ•°æ®é›†ä¸€è‡´æ€§æ—¶å‡ºé”™: {e}")
        return False

def print_overall_metrics(all_metrics):
    """æ‰“å°ç»¼åˆè¯„ä¼°ç»“æœ"""
    print("\n--- ç»¼åˆè¯„ä¼°ç»“æœ ===")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤šä¸ªæ•°æ®é›†çš„å¯¹æ¯”
    if any('val' in metrics and 'train' in metrics for metrics in all_metrics.values()):
        # è¿™æ˜¯å¤šä¸ªæ•°æ®é›†çš„å¯¹æ¯”
        print("=== å¤šæ•°æ®é›†æ€§èƒ½å¯¹æ¯” ===")
        
        # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
        print(f"{'æ•°æ®é›†':<20} {'éªŒè¯é›†Dice':<15} {'éªŒè¯é›†IoU':<15} {'è®­ç»ƒé›†Dice':<15} {'è®­ç»ƒé›†IoU':<15}")
        print("-" * 80)
        
        for dataset_name, metrics in all_metrics.items():
            if 'val' in metrics and 'train' in metrics:
                val_dice = metrics['val'].get('OverallDice', metrics['val']['Dice'].mean())
                val_iou = metrics['val'].get('OverallIoU', metrics['val']['IoU'].mean())
                train_dice = metrics['train'].get('OverallDice', metrics['train']['Dice'].mean())
                train_iou = metrics['train'].get('OverallIoU', metrics['train']['IoU'].mean())
                
                print(f"{dataset_name:<20} {val_dice:<15.4f} {val_iou:<15.4f} {train_dice:<15.4f} {train_iou:<15.4f}")
        
        # è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„æ€»ä½“å¹³å‡
        print("-" * 80)
        total_val_dice = sum(metrics['val'].get('OverallDice', metrics['val']['Dice'].mean()) for metrics in all_metrics.values() if 'val' in metrics)
        total_val_iou = sum(metrics['val'].get('OverallIoU', metrics['val']['IoU'].mean()) for metrics in all_metrics.values() if 'val' in metrics)
        total_train_dice = sum(metrics['train'].get('OverallDice', metrics['train']['Dice'].mean()) for metrics in all_metrics.values() if 'train' in metrics)
        total_train_iou = sum(metrics['train'].get('OverallIoU', metrics['train']['IoU'].mean()) for metrics in all_metrics.values() if 'train' in metrics)
        
        num_datasets = len(all_metrics)
        avg_val_dice = total_val_dice / num_datasets
        avg_val_iou = total_val_iou / num_datasets
        avg_train_dice = total_train_dice / num_datasets
        avg_train_iou = total_train_iou / num_datasets
        
        print(f"{'æ€»ä½“å¹³å‡':<20} {avg_val_dice:<15.4f} {avg_val_iou:<15.4f} {avg_train_dice:<15.4f} {avg_train_iou:<15.4f}")
        
    else:
        # è¿™æ˜¯å•ä¸ªæ•°æ®é›†çš„å¤šä¸ªsplitå¯¹æ¯”
        print("=== å•æ•°æ®é›†å¤šSplitå¯¹æ¯” ===")
        
        # è®¡ç®—æ€»ä½“å¹³å‡
        total_dice = 0
        total_iou = 0
        total_samples = 0
        
        for split, metrics in all_metrics.items():
            print(f"\n{split} é›†æŒ‡æ ‡:")
            if 'ClassNames' in metrics:
                for j in range(len(metrics['ClassNames'])):
                    cname = metrics['ClassNames'].get(str(j), f'class_{j}')
                    dice_mean = metrics['Dice'][:,j].mean()
                    iou_mean = metrics['IoU'][:,j].mean()
                    dice_std = metrics['Dice'][:,j].std()
                    iou_std = metrics['IoU'][:,j].std()
                    print(f"  {cname}: Dice: {dice_mean:.4f}Â±{dice_std:.4f}, IoU: {iou_mean:.4f}Â±{iou_std:.4f}")
                
                # ä½¿ç”¨æ–°çš„æŒ‡æ ‡ç»“æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'OverallDice' in metrics:
                    split_dice = metrics['OverallDice']
                    split_iou = metrics['OverallIoU']
                    split_dice_std = metrics.get('DiceStd', 0.0)
                    split_iou_std = metrics.get('IoUStd', 0.0)
                    print(f"  {split} é›†å¹³å‡: Dice: {split_dice:.4f}Â±{split_dice_std:.4f}, IoU: {split_iou:.4f}Â±{split_iou_std:.4f}")
                else:
                    split_dice = metrics['Dice'].mean()
                    split_iou = metrics['IoU'].mean()
                    print(f"  {split} é›†å¹³å‡: Dice: {split_dice:.4f}, IoU: {split_iou:.4f}")
                
                # ç´¯åŠ ç”¨äºè®¡ç®—æ€»ä½“å¹³å‡
                total_dice += split_dice
                total_iou += split_iou
                total_samples += 1
        
        # è®¡ç®—æ€»ä½“å¹³å‡
        if total_samples > 0:
            overall_dice = total_dice / total_samples
            overall_iou = total_iou / total_samples
            print(f"\n=== æ€»ä½“å¹³å‡æ€§èƒ½ ===")
            print(f"averageDice: {overall_dice:.4f}")
            print(f"averageIoU: {overall_iou:.4f}")



# ä¸»ç¨‹åºå…¥å£
if __name__ == '__main__':
    print("--- æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°è„šæœ¬ ---")
    base_config = build_base_config_from_module(base_config_module)
    evaluation_config = get_evaluation_config()
    evaluate_model(evaluation_config, base_config) 